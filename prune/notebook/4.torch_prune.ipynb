{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型剪枝Torch实践\n",
    "Pytorch在1.4.0版本开始，加入了剪枝操作，在`torch.nn.utils.prune`模块中，本教程按照剪枝范围划分，将其分以下几种剪枝方式:\n",
    "- 局部剪枝（Local Pruning）\n",
    "  - 结构化剪枝\n",
    "    - 随机结构化剪枝（random_structured）\n",
    "    - 范数结构化剪枝（ln_structured）\n",
    "  - 非结构化剪枝\n",
    "    - 随机非结构化剪枝（random_unstructured）\n",
    "    - 范数非结构化剪枝（l1_unstructured）\n",
    "- 全局剪枝（Global Pruning）\n",
    "  - 非结构化剪枝（global_unstructured）\n",
    "- 自定义剪枝（Custom  Pruning）\n",
    "  \n",
    "**注：** 全局剪枝只有非结构化剪枝方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、局部剪枝\n",
    "首先介绍局部剪枝（Local Pruning）方式，指的是对网络的单个层或局部范围内进行剪枝。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 结构化剪枝\n",
    "按照剪枝方式划分，可以分为结构化剪枝和非结构化剪枝方式。非结构化剪枝会随机地将一些权重参数变为0，结构化剪枝则将某个维度某些通道变成0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 随机结构化剪枝（random_structured）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个经典的LeNet网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个LeNet网络\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 120]          30,840\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 打印模型结构\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[[[ 0.0574,  0.1150,  0.0779,  0.1793,  0.0816],\n",
      "          [-0.1272, -0.0605,  0.0163, -0.1025,  0.0098],\n",
      "          [ 0.0987, -0.1823,  0.0073, -0.0604,  0.1567],\n",
      "          [-0.1940, -0.1440, -0.1294,  0.0584,  0.0656],\n",
      "          [ 0.1686,  0.1801,  0.1256,  0.1649, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1816,  0.1502,  0.1685,  0.1069],\n",
      "          [ 0.0477,  0.0950,  0.1366,  0.1627,  0.1863],\n",
      "          [ 0.1487, -0.0796,  0.0153, -0.1570,  0.0340],\n",
      "          [ 0.1867, -0.1581, -0.1164, -0.0527,  0.1751],\n",
      "          [-0.0663,  0.1908,  0.0294, -0.0944,  0.1881]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1066,  0.1722, -0.1570, -0.0281, -0.0394],\n",
      "          [-0.1610,  0.0415, -0.1289,  0.0632, -0.0832],\n",
      "          [-0.0289, -0.0013, -0.1875,  0.1034, -0.1127],\n",
      "          [-0.0196,  0.1563, -0.1115, -0.0844, -0.1174],\n",
      "          [ 0.0336, -0.1276, -0.0782,  0.0894,  0.0307]]],\n",
      "\n",
      "\n",
      "        [[[-0.1679, -0.1715, -0.0894, -0.1393,  0.0144],\n",
      "          [-0.0478,  0.0460, -0.0485, -0.1887,  0.1669],\n",
      "          [ 0.0753, -0.0656,  0.1934, -0.0307, -0.0408],\n",
      "          [-0.1172, -0.0188,  0.0768, -0.1355,  0.0740],\n",
      "          [-0.1271, -0.1591, -0.0429,  0.0473, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1042,  0.1549, -0.1653,  0.1695,  0.1111],\n",
      "          [-0.0731,  0.1611, -0.1008, -0.0504,  0.1167],\n",
      "          [-0.0230,  0.0030, -0.0516,  0.1702,  0.1578],\n",
      "          [-0.1955,  0.0364,  0.1371, -0.0234,  0.0713],\n",
      "          [ 0.1620,  0.1629, -0.0325, -0.1376,  0.0212]]],\n",
      "\n",
      "\n",
      "        [[[-0.1092, -0.0816, -0.1193,  0.0967,  0.0456],\n",
      "          [-0.1936,  0.0726,  0.1186, -0.0313,  0.1051],\n",
      "          [-0.1385,  0.1876, -0.1892,  0.0505,  0.1376],\n",
      "          [-0.0073,  0.0874,  0.1560,  0.0017, -0.1283],\n",
      "          [-0.0441,  0.1064,  0.1536,  0.0694,  0.0684]]]], device='cuda:0',\n",
      "       requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([-0.1801,  0.0865,  0.1263, -0.1939, -0.0692,  0.0260], device='cuda:0',\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# 打印第一个卷积层的参数\n",
    "module = model.conv1\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 打印module中的属性张量named_buffers，初始时为空列表\n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# 打印模型的状态字典，状态字典里包含了所有的参数\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个参数: module, 代表要进行剪枝的特定模块, 这里指的是module=model.conv1,\n",
    "#             说明这里要对第一个卷积层执行剪枝.\n",
    "# 第二个参数: name, 代表要对选中的模块中的哪些参数执行剪枝.\n",
    "#             这里设定为name=\"weight\", 说明是对网络中的weight剪枝, 而不对bias剪枝.\n",
    "# 第三个参数: amount, 代表要对模型中特定比例或绝对数量的参数执行剪枝.\n",
    "#             amount是一个介于0.0-1.0的float数值,代表比例, 或者一个正整数，代表指定剪裁掉多少个参数.\n",
    "# 第四个参数: dim, 代表要进行剪枝通道(channel)的维度索引.\n",
    "#            \n",
    "\n",
    "prune.random_structured(module, name=\"weight\", amount=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# 再次打印模型的状态字典，观察conv1层\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([-0.1801,  0.0865,  0.1263, -0.1939, -0.0692,  0.0260], device='cuda:0',\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0574,  0.1150,  0.0779,  0.1793,  0.0816],\n",
      "          [-0.1272, -0.0605,  0.0163, -0.1025,  0.0098],\n",
      "          [ 0.0987, -0.1823,  0.0073, -0.0604,  0.1567],\n",
      "          [-0.1940, -0.1440, -0.1294,  0.0584,  0.0656],\n",
      "          [ 0.1686,  0.1801,  0.1256,  0.1649, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1816,  0.1502,  0.1685,  0.1069],\n",
      "          [ 0.0477,  0.0950,  0.1366,  0.1627,  0.1863],\n",
      "          [ 0.1487, -0.0796,  0.0153, -0.1570,  0.0340],\n",
      "          [ 0.1867, -0.1581, -0.1164, -0.0527,  0.1751],\n",
      "          [-0.0663,  0.1908,  0.0294, -0.0944,  0.1881]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1066,  0.1722, -0.1570, -0.0281, -0.0394],\n",
      "          [-0.1610,  0.0415, -0.1289,  0.0632, -0.0832],\n",
      "          [-0.0289, -0.0013, -0.1875,  0.1034, -0.1127],\n",
      "          [-0.0196,  0.1563, -0.1115, -0.0844, -0.1174],\n",
      "          [ 0.0336, -0.1276, -0.0782,  0.0894,  0.0307]]],\n",
      "\n",
      "\n",
      "        [[[-0.1679, -0.1715, -0.0894, -0.1393,  0.0144],\n",
      "          [-0.0478,  0.0460, -0.0485, -0.1887,  0.1669],\n",
      "          [ 0.0753, -0.0656,  0.1934, -0.0307, -0.0408],\n",
      "          [-0.1172, -0.0188,  0.0768, -0.1355,  0.0740],\n",
      "          [-0.1271, -0.1591, -0.0429,  0.0473, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1042,  0.1549, -0.1653,  0.1695,  0.1111],\n",
      "          [-0.0731,  0.1611, -0.1008, -0.0504,  0.1167],\n",
      "          [-0.0230,  0.0030, -0.0516,  0.1702,  0.1578],\n",
      "          [-0.1955,  0.0364,  0.1371, -0.0234,  0.0713],\n",
      "          [ 0.1620,  0.1629, -0.0325, -0.1376,  0.0212]]],\n",
      "\n",
      "\n",
      "        [[[-0.1092, -0.0816, -0.1193,  0.0967,  0.0456],\n",
      "          [-0.1936,  0.0726,  0.1186, -0.0313,  0.1051],\n",
      "          [-0.1385,  0.1876, -0.1892,  0.0505,  0.1376],\n",
      "          [-0.0073,  0.0874,  0.1560,  0.0017, -0.1283],\n",
      "          [-0.0441,  0.1064,  0.1536,  0.0694,  0.0684]]]], device='cuda:0',\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# 再次打印module中的属性张量named_buffers\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]], device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "# 再次打印module中的属性张量named_buffers\n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 经过剪枝操作后, 原始的权重矩阵weight变成了weight_orig. 并且剪枝前打印为空列表的module.named_buffers(), 现在多了weight_mask参数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0574,  0.1150,  0.0779,  0.1793,  0.0816],\n",
      "          [-0.1272, -0.0605,  0.0163, -0.1025,  0.0098],\n",
      "          [ 0.0987, -0.1823,  0.0073, -0.0604,  0.1567],\n",
      "          [-0.1940, -0.1440, -0.1294,  0.0584,  0.0656],\n",
      "          [ 0.1686,  0.1801,  0.1256,  0.1649, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1816,  0.1502,  0.1685,  0.1069],\n",
      "          [ 0.0477,  0.0950,  0.1366,  0.1627,  0.1863],\n",
      "          [ 0.1487, -0.0796,  0.0153, -0.1570,  0.0340],\n",
      "          [ 0.1867, -0.1581, -0.1164, -0.0527,  0.1751],\n",
      "          [-0.0663,  0.1908,  0.0294, -0.0944,  0.1881]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1066,  0.1722, -0.1570, -0.0281, -0.0394],\n",
      "          [-0.1610,  0.0415, -0.1289,  0.0632, -0.0832],\n",
      "          [-0.0289, -0.0013, -0.1875,  0.1034, -0.1127],\n",
      "          [-0.0196,  0.1563, -0.1115, -0.0844, -0.1174],\n",
      "          [ 0.0336, -0.1276, -0.0782,  0.0894,  0.0307]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1042,  0.1549, -0.1653,  0.1695,  0.1111],\n",
      "          [-0.0731,  0.1611, -0.1008, -0.0504,  0.1167],\n",
      "          [-0.0230,  0.0030, -0.0516,  0.1702,  0.1578],\n",
      "          [-0.1955,  0.0364,  0.1371, -0.0234,  0.0713],\n",
      "          [ 0.1620,  0.1629, -0.0325, -0.1376,  0.0212]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 打印module.weight, 看看发现了什么？\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 经过剪枝操作后， 原始的weight变成了weight_orig，并存放在named_parameters中, 对应的剪枝矩阵存放在weight_mask中, 将weight_mask视作掩码张量, 再和weight_orig相乘的结果就存放在了weight中."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意:** 剪枝操作后的weight已经不再是module的参数(parameter), 而只是module的一个属性(attribute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每一次剪枝操作, 模型都会对应一个具体的_forward_pre_hooks函数用于剪枝，该函数存放执行过的剪枝操作."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(6, <torch.nn.utils.prune.RandomStructured object at 0x7f8216027610>)])\n"
     ]
    }
   ],
   "source": [
    "# 打印_forward_pre_hooks\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 范数结构化剪枝（ln_structured）\n",
    "一个模型的参数可以执行多次剪枝操作，这种操作被称为迭代剪枝（Iterative Pruning）。上述步骤已经对conv1进行了随机结构化剪枝，接下来对其再进行范数结构化剪枝，看看会发生什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model state_dict keys:\n",
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " module named_parameters:\n",
      "[('bias', Parameter containing:\n",
      "tensor([-0.1801,  0.0865,  0.1263, -0.1939, -0.0692,  0.0260], device='cuda:0',\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0574,  0.1150,  0.0779,  0.1793,  0.0816],\n",
      "          [-0.1272, -0.0605,  0.0163, -0.1025,  0.0098],\n",
      "          [ 0.0987, -0.1823,  0.0073, -0.0604,  0.1567],\n",
      "          [-0.1940, -0.1440, -0.1294,  0.0584,  0.0656],\n",
      "          [ 0.1686,  0.1801,  0.1256,  0.1649, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1816,  0.1502,  0.1685,  0.1069],\n",
      "          [ 0.0477,  0.0950,  0.1366,  0.1627,  0.1863],\n",
      "          [ 0.1487, -0.0796,  0.0153, -0.1570,  0.0340],\n",
      "          [ 0.1867, -0.1581, -0.1164, -0.0527,  0.1751],\n",
      "          [-0.0663,  0.1908,  0.0294, -0.0944,  0.1881]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1066,  0.1722, -0.1570, -0.0281, -0.0394],\n",
      "          [-0.1610,  0.0415, -0.1289,  0.0632, -0.0832],\n",
      "          [-0.0289, -0.0013, -0.1875,  0.1034, -0.1127],\n",
      "          [-0.0196,  0.1563, -0.1115, -0.0844, -0.1174],\n",
      "          [ 0.0336, -0.1276, -0.0782,  0.0894,  0.0307]]],\n",
      "\n",
      "\n",
      "        [[[-0.1679, -0.1715, -0.0894, -0.1393,  0.0144],\n",
      "          [-0.0478,  0.0460, -0.0485, -0.1887,  0.1669],\n",
      "          [ 0.0753, -0.0656,  0.1934, -0.0307, -0.0408],\n",
      "          [-0.1172, -0.0188,  0.0768, -0.1355,  0.0740],\n",
      "          [-0.1271, -0.1591, -0.0429,  0.0473, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1042,  0.1549, -0.1653,  0.1695,  0.1111],\n",
      "          [-0.0731,  0.1611, -0.1008, -0.0504,  0.1167],\n",
      "          [-0.0230,  0.0030, -0.0516,  0.1702,  0.1578],\n",
      "          [-0.1955,  0.0364,  0.1371, -0.0234,  0.0713],\n",
      "          [ 0.1620,  0.1629, -0.0325, -0.1376,  0.0212]]],\n",
      "\n",
      "\n",
      "        [[[-0.1092, -0.0816, -0.1193,  0.0967,  0.0456],\n",
      "          [-0.1936,  0.0726,  0.1186, -0.0313,  0.1051],\n",
      "          [-0.1385,  0.1876, -0.1892,  0.0505,  0.1376],\n",
      "          [-0.0073,  0.0874,  0.1560,  0.0017, -0.1283],\n",
      "          [-0.0441,  0.1064,  0.1536,  0.0694,  0.0684]]]], device='cuda:0',\n",
      "       requires_grad=True))]\n",
      "**************************************************\n",
      " module named_buffers:\n",
      "[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]], device='cuda:0'))]\n",
      "**************************************************\n",
      " module weight:\n",
      "tensor([[[[ 0.0574,  0.1150,  0.0779,  0.1793,  0.0816],\n",
      "          [-0.1272, -0.0605,  0.0163, -0.1025,  0.0098],\n",
      "          [ 0.0987, -0.1823,  0.0073, -0.0604,  0.1567],\n",
      "          [-0.1940, -0.1440, -0.1294,  0.0584,  0.0656],\n",
      "          [ 0.1686,  0.1801,  0.1256,  0.1649, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1816,  0.1502,  0.1685,  0.1069],\n",
      "          [ 0.0477,  0.0950,  0.1366,  0.1627,  0.1863],\n",
      "          [ 0.1487, -0.0796,  0.0153, -0.1570,  0.0340],\n",
      "          [ 0.1867, -0.1581, -0.1164, -0.0527,  0.1751],\n",
      "          [-0.0663,  0.1908,  0.0294, -0.0944,  0.1881]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      " module _forward_pre_hooks:\n",
      "OrderedDict([(7, <torch.nn.utils.prune.PruningContainer object at 0x7f8216027ee0>)])\n"
     ]
    }
   ],
   "source": [
    "# 第一个参数: module, 代表要进行剪枝的特定模块, 这里指的是module=model.conv1,\n",
    "#             说明这里要对第一个卷积层执行剪枝.\n",
    "# 第二个参数: name, 代表要对选中的模块中的哪些参数执行剪枝.\n",
    "#             这里设定为name=\"weight\", 说明是对网络中的weight剪枝, 而不对bias剪枝.\n",
    "# 第三个参数: amount, 代表要对模型中特定比例或绝对数量的参数执行剪枝.\n",
    "#             amount是一个介于0.0-1.0的float数值,代表比例, 或者一个正整数，代表指定剪裁掉多少个参数.\n",
    "# 第四个参数: n, 代表范数类型，这里n=2代表是L2范数.\n",
    "# 第五个参数: dim, 代表要进行剪枝通道(channel)的维度索引.\n",
    "\n",
    "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module weight:\")\n",
    "print(module.weight)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module _forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论：迭代剪枝相当于把多个剪枝核序列化成一个剪枝核, 新的 mask 矩阵与旧的 mask 矩阵的结合使用 PruningContainer 中的 compute_mask 方法，最后只有一个weight_orig和weight_mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "怎么能看到所有的剪枝历史呢？ module._forward_pre_hooks是一个用于在模型的前向传播之前执行自定义操作的机制，这里记录了执行过的剪枝方法."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.nn.utils.prune.RandomStructured object at 0x7f8216027610>, <torch.nn.utils.prune.LnStructured object at 0x7f8216024a90>]\n"
     ]
    }
   ],
   "source": [
    "# 打印剪枝历史\n",
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == \"weight\":  \n",
    "        break\n",
    "\n",
    "print(list(hook))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 随机非结构化剪枝（random_unstructured）\n",
    "可以对模型的任意子结构进行剪枝操作, 除了在weight上面剪枝, 还可以对bias进行剪枝."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model state_dict keys:\n",
      "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " module named_parameters:\n",
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0574,  0.1150,  0.0779,  0.1793,  0.0816],\n",
      "          [-0.1272, -0.0605,  0.0163, -0.1025,  0.0098],\n",
      "          [ 0.0987, -0.1823,  0.0073, -0.0604,  0.1567],\n",
      "          [-0.1940, -0.1440, -0.1294,  0.0584,  0.0656],\n",
      "          [ 0.1686,  0.1801,  0.1256,  0.1649, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1816,  0.1502,  0.1685,  0.1069],\n",
      "          [ 0.0477,  0.0950,  0.1366,  0.1627,  0.1863],\n",
      "          [ 0.1487, -0.0796,  0.0153, -0.1570,  0.0340],\n",
      "          [ 0.1867, -0.1581, -0.1164, -0.0527,  0.1751],\n",
      "          [-0.0663,  0.1908,  0.0294, -0.0944,  0.1881]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1066,  0.1722, -0.1570, -0.0281, -0.0394],\n",
      "          [-0.1610,  0.0415, -0.1289,  0.0632, -0.0832],\n",
      "          [-0.0289, -0.0013, -0.1875,  0.1034, -0.1127],\n",
      "          [-0.0196,  0.1563, -0.1115, -0.0844, -0.1174],\n",
      "          [ 0.0336, -0.1276, -0.0782,  0.0894,  0.0307]]],\n",
      "\n",
      "\n",
      "        [[[-0.1679, -0.1715, -0.0894, -0.1393,  0.0144],\n",
      "          [-0.0478,  0.0460, -0.0485, -0.1887,  0.1669],\n",
      "          [ 0.0753, -0.0656,  0.1934, -0.0307, -0.0408],\n",
      "          [-0.1172, -0.0188,  0.0768, -0.1355,  0.0740],\n",
      "          [-0.1271, -0.1591, -0.0429,  0.0473, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1042,  0.1549, -0.1653,  0.1695,  0.1111],\n",
      "          [-0.0731,  0.1611, -0.1008, -0.0504,  0.1167],\n",
      "          [-0.0230,  0.0030, -0.0516,  0.1702,  0.1578],\n",
      "          [-0.1955,  0.0364,  0.1371, -0.0234,  0.0713],\n",
      "          [ 0.1620,  0.1629, -0.0325, -0.1376,  0.0212]]],\n",
      "\n",
      "\n",
      "        [[[-0.1092, -0.0816, -0.1193,  0.0967,  0.0456],\n",
      "          [-0.1936,  0.0726,  0.1186, -0.0313,  0.1051],\n",
      "          [-0.1385,  0.1876, -0.1892,  0.0505,  0.1376],\n",
      "          [-0.0073,  0.0874,  0.1560,  0.0017, -0.1283],\n",
      "          [-0.0441,  0.1064,  0.1536,  0.0694,  0.0684]]]], device='cuda:0',\n",
      "       requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([-0.1801,  0.0865,  0.1263, -0.1939, -0.0692,  0.0260], device='cuda:0',\n",
      "       requires_grad=True))]\n",
      "**************************************************\n",
      " module named_buffers:\n",
      "[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]], device='cuda:0')), ('bias_mask', tensor([1., 1., 1., 1., 1., 0.], device='cuda:0'))]\n",
      "**************************************************\n",
      " module bias:\n",
      "tensor([-0.1801,  0.0865,  0.1263, -0.1939, -0.0692,  0.0000], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      " module _forward_pre_hooks:\n",
      "OrderedDict([(7, <torch.nn.utils.prune.PruningContainer object at 0x7f8216027ee0>), (8, <torch.nn.utils.prune.RandomUnstructured object at 0x7f82160254b0>)])\n"
     ]
    }
   ],
   "source": [
    "# 第一个参数: module, 代表要进行剪枝的特定模块, 这里指的是module=model.conv1,\n",
    "#             说明这里要对第一个卷积层执行剪枝.\n",
    "# 第二个参数: name, 代表要对选中的模块中的哪些参数执行剪枝.\n",
    "#             这里设定为name=\"weight\", 说明是对网络中的weight剪枝, 而不对bias剪枝.\n",
    "# 第三个参数: amount, 代表要对模型中特定比例或绝对数量的参数执行剪枝.\n",
    "#             amount是一个介于0.0-1.0的float数值,代表比例, 或者一个正整数，代表指定剪裁掉多少个参数.\n",
    "\n",
    "prune.random_unstructured(module, name=\"bias\", amount=1)\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module bias:\")\n",
    "print(module.bias)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module _forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 在module的不同参数集合上应用不同的剪枝策略, 可以发现在模型参数state_dict和named_parameters中不仅仅有了weight_orig, 也有了bias_orig. 在参数named_buffers中, 也同时出现了weight_mask和bias_mask. \n",
    "最后, 因为我们在两类参数上应用了两种不同的剪枝函数, 因此_forward_pre_hooks中也打印出了2个不同的函数结果."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 范数非结构化剪枝（l1_unstructured）\n",
    "前面介绍了对指定的conv1层的weight和bias进行了不同方法的剪枝，那么能不能支持同时对多层网络的特定参数进行剪枝呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model state_dict keys:\n",
      "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias_orig', 'conv2.bias_mask', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " module named_parameters:\n",
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0574,  0.1150,  0.0779,  0.1793,  0.0816],\n",
      "          [-0.1272, -0.0605,  0.0163, -0.1025,  0.0098],\n",
      "          [ 0.0987, -0.1823,  0.0073, -0.0604,  0.1567],\n",
      "          [-0.1940, -0.1440, -0.1294,  0.0584,  0.0656],\n",
      "          [ 0.1686,  0.1801,  0.1256,  0.1649, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1816,  0.1502,  0.1685,  0.1069],\n",
      "          [ 0.0477,  0.0950,  0.1366,  0.1627,  0.1863],\n",
      "          [ 0.1487, -0.0796,  0.0153, -0.1570,  0.0340],\n",
      "          [ 0.1867, -0.1581, -0.1164, -0.0527,  0.1751],\n",
      "          [-0.0663,  0.1908,  0.0294, -0.0944,  0.1881]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1066,  0.1722, -0.1570, -0.0281, -0.0394],\n",
      "          [-0.1610,  0.0415, -0.1289,  0.0632, -0.0832],\n",
      "          [-0.0289, -0.0013, -0.1875,  0.1034, -0.1127],\n",
      "          [-0.0196,  0.1563, -0.1115, -0.0844, -0.1174],\n",
      "          [ 0.0336, -0.1276, -0.0782,  0.0894,  0.0307]]],\n",
      "\n",
      "\n",
      "        [[[-0.1679, -0.1715, -0.0894, -0.1393,  0.0144],\n",
      "          [-0.0478,  0.0460, -0.0485, -0.1887,  0.1669],\n",
      "          [ 0.0753, -0.0656,  0.1934, -0.0307, -0.0408],\n",
      "          [-0.1172, -0.0188,  0.0768, -0.1355,  0.0740],\n",
      "          [-0.1271, -0.1591, -0.0429,  0.0473, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1042,  0.1549, -0.1653,  0.1695,  0.1111],\n",
      "          [-0.0731,  0.1611, -0.1008, -0.0504,  0.1167],\n",
      "          [-0.0230,  0.0030, -0.0516,  0.1702,  0.1578],\n",
      "          [-0.1955,  0.0364,  0.1371, -0.0234,  0.0713],\n",
      "          [ 0.1620,  0.1629, -0.0325, -0.1376,  0.0212]]],\n",
      "\n",
      "\n",
      "        [[[-0.1092, -0.0816, -0.1193,  0.0967,  0.0456],\n",
      "          [-0.1936,  0.0726,  0.1186, -0.0313,  0.1051],\n",
      "          [-0.1385,  0.1876, -0.1892,  0.0505,  0.1376],\n",
      "          [-0.0073,  0.0874,  0.1560,  0.0017, -0.1283],\n",
      "          [-0.0441,  0.1064,  0.1536,  0.0694,  0.0684]]]], device='cuda:0',\n",
      "       requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([-0.1801,  0.0865,  0.1263, -0.1939, -0.0692,  0.0260], device='cuda:0',\n",
      "       requires_grad=True))]\n",
      "**************************************************\n",
      " module named_buffers:\n",
      "[('weight_mask', tensor([[[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]], device='cuda:0')), ('bias_mask', tensor([1., 1., 1., 1., 0., 0.], device='cuda:0'))]\n",
      "**************************************************\n",
      " module weight:\n",
      "tensor([[[[ 0.0574,  0.1150,  0.0779,  0.1793,  0.0816],\n",
      "          [-0.1272, -0.0605,  0.0163, -0.1025,  0.0098],\n",
      "          [ 0.0987, -0.1823,  0.0073, -0.0604,  0.1567],\n",
      "          [-0.1940, -0.1440, -0.1294,  0.0584,  0.0656],\n",
      "          [ 0.1686,  0.1801,  0.1256,  0.1649, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1816,  0.1502,  0.1685,  0.1069],\n",
      "          [ 0.0477,  0.0950,  0.1366,  0.1627,  0.1863],\n",
      "          [ 0.1487, -0.0796,  0.0153, -0.1570,  0.0340],\n",
      "          [ 0.1867, -0.1581, -0.1164, -0.0527,  0.1751],\n",
      "          [-0.0663,  0.1908,  0.0294, -0.0944,  0.1881]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      " module bias:\n",
      "tensor([-0.1801,  0.0865,  0.1263, -0.1939, -0.0000,  0.0000], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      " module _forward_pre_hooks:\n",
      "OrderedDict([(7, <torch.nn.utils.prune.PruningContainer object at 0x7f8216027ee0>), (9, <torch.nn.utils.prune.PruningContainer object at 0x7f8216025a50>)])\n"
     ]
    }
   ],
   "source": [
    "# 对于模型进行分模块参数的剪枝\n",
    "for n, m in model.named_modules():\n",
    "    # 对模型中所有的卷积层执行l1_unstructured剪枝操作, 选取20%的参数剪枝\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(m, name=\"bias\", amount=0.2)\n",
    "    # 对模型中所有全连接层执行ln_structured剪枝操作, 选取40%的参数剪枝\n",
    "    # elif isinstance(module, torch.nn.Linear):\n",
    "    #     prune.random_structured(module, name=\"weight\", amount=0.4,dim=0)\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module weight:\")\n",
    "print(module.weight)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module bias:\")\n",
    "print(module.bias)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module _forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来对模型的weight进行剪枝永久化操作remove，经过前面的剪枝步骤， 原来的weight 已经变成'weight_orig', 而 weight 是'weight_orig' 与 mask 矩阵相乘后的结果，变成一个属性,请观察在remove之后发生了哪些变化？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      " model state_dict keys:\n",
      "odict_keys(['conv1.bias', 'conv1.weight', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " model named_parameters:\n",
      "[('bias', Parameter containing:\n",
      "tensor([-0.1801,  0.0865,  0.1263, -0.1939, -0.0000,  0.0000], device='cuda:0',\n",
      "       requires_grad=True)), ('weight', Parameter containing:\n",
      "tensor([[[[ 0.0574,  0.1150,  0.0779,  0.1793,  0.0816],\n",
      "          [-0.1272, -0.0605,  0.0163, -0.1025,  0.0098],\n",
      "          [ 0.0987, -0.1823,  0.0073, -0.0604,  0.1567],\n",
      "          [-0.1940, -0.1440, -0.1294,  0.0584,  0.0656],\n",
      "          [ 0.1686,  0.1801,  0.1256,  0.1649, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[-0.1933,  0.1816,  0.1502,  0.1685,  0.1069],\n",
      "          [ 0.0477,  0.0950,  0.1366,  0.1627,  0.1863],\n",
      "          [ 0.1487, -0.0796,  0.0153, -0.1570,  0.0340],\n",
      "          [ 0.1867, -0.1581, -0.1164, -0.0527,  0.1751],\n",
      "          [-0.0663,  0.1908,  0.0294, -0.0944,  0.1881]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       requires_grad=True))]\n",
      "**************************************************\n",
      " model named_buffers:\n",
      "[]\n",
      "**************************************************\n",
      " model forward_pre_hooks:\n",
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "# 对module的weight执行剪枝永久化操作remove\n",
    "for n, m in model.named_modules():\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        prune.remove(m, 'bias')\n",
    "\n",
    "# 对conv1的weight执行剪枝永久化操作remove\n",
    "prune.remove(module, 'weight')\n",
    "print('*'*50)\n",
    "\n",
    "# 将剪枝后的模型的状态字典打印出来\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 再次打印模型参数\n",
    "print(\" model named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "# 再次打印模型mask buffers参数\n",
    "print(\" model named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "# 再次打印模型的_forward_pre_hooks\n",
    "print(\" model forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 对模型的weight和bias执行remove操作后, 模型参数集合中weight_orig和bias_orig消失, 变成了weight和bias, 说明剪枝已经永久化生效. 对于named_buffers张量打印可以看出, 只剩下[], 因为针对weight和bias做掩码的weight_mask和bias——mask已经生效完毕, 不再需要保留了. \n",
    "同理, 在_forward_pre_hooks中也只剩下空字典了.weight和bias 又变成了 parameters, 剪枝变成永久化."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.全局剪枝(GLobal pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面已经介绍了局部剪枝（Local Pruning）的四种方法，但很大程度上需要根据自己的经验来决定对某一层网络进行剪枝.\n",
    "更通用的剪枝策略是采用全局剪枝(global pruning),  从整体网络的角度进行剪枝，采用全局剪枝后, 不同的层被剪掉的百分比可能不同.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.bias', 'conv2.weight_orig', 'conv2.weight_mask', 'fc1.bias', 'fc1.weight_orig', 'fc1.weight_mask', 'fc2.bias', 'fc2.weight_orig', 'fc2.weight_mask', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device=device)\n",
    "\n",
    "# 首先打印初始化模型的状态字典\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "# 构建参数集合, 决定哪些层, 哪些参数集合参与剪枝\n",
    "parameters_to_prune = (\n",
    "            (model.conv1, 'weight'),\n",
    "            (model.conv2, 'weight'),\n",
    "            (model.fc1, 'weight'),\n",
    "            (model.fc2, 'weight'))\n",
    "\n",
    "# 调用prune中的全局剪枝函数global_unstructured执行剪枝操作\n",
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2)\n",
    "\n",
    "# 打印剪枝后的模型的状态字典\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对模型剪枝后, 不同的层会有不同比例的权重参数被剪掉, 利用代码打印出来看看:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 9.33%\n",
      "Sparsity in conv2.weight: 16.25%\n",
      "Sparsity in fc1.weight: 21.85%\n",
      "Sparsity in fc2.weight: 15.41%\n",
      "Global sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv1.weight == 0))\n",
    "    / float(model.conv1.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv2.weight == 0))\n",
    "    / float(model.conv2.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.fc1.weight == 0))\n",
    "    / float(model.fc1.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.fc2.weight == 0))\n",
    "    / float(model.fc2.weight.nelement())\n",
    "    ))\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv1.weight == 0)\n",
    "               + torch.sum(model.conv2.weight == 0)\n",
    "               + torch.sum(model.fc1.weight == 0)\n",
    "               + torch.sum(model.fc2.weight == 0))\n",
    "         / float(model.conv1.weight.nelement()\n",
    "               + model.conv2.weight.nelement()\n",
    "               + model.fc1.weight.nelement()\n",
    "               + model.fc2.weight.nelement())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 当采用全局剪枝策略的时候(假定20%比例参数参与剪枝), 仅保证模型总体参数量的20%被剪枝掉, 具体到每一层的情况则由模型的具体参数分布情况来定.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.用户自定义剪枝(Custom pruning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剪枝模型通过继承class BasePruningMethod()来执行剪枝, 内部有若干方法: call, apply_mask, apply, prune, remove等等.必须实现__init__（构造函数）和compute_mask两个函数才能完成自定义的剪枝规则设定."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义剪枝方法的类, 一定要继承prune.BasePruningMethod\n",
    "class custom_prune(prune.BasePruningMethod):\n",
    "    # 指定此技术实现的修剪类型（支持的选项为global、 structured和unstructured）\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    # 内部实现compute_mask函数, 完定义的剪枝规则, 本质上就是如何去mask掉权重参数\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        # 此处定义的规则是每隔一个参数就遮掩掉一个, 最终参与剪枝的参数量的50%被mask掉\n",
    "        mask.view(-1)[::2] = 0\n",
    "        return mask\n",
    "\n",
    "# 自定义剪枝方法的函数, 内部直接调用剪枝类的方法apply\n",
    "def custome_unstructured_pruning(module, name):\n",
    "    custom_prune.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0')\n",
      "14.882087707519531 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 实例化模型类\n",
    "model = LeNet().to(device=device)\n",
    "\n",
    "start = time.time()\n",
    "# 调用自定义剪枝方法的函数, 对model中的第1个全连接层fc1中的偏置bias执行自定义剪枝\n",
    "custome_unstructured_pruning(model.fc1, name=\"bias\")\n",
    "\n",
    "# 剪枝成功的最大标志, 就是拥有了bias_mask参数\n",
    "print(model.fc1.bias_mask)\n",
    "\n",
    "# 打印一下自定义剪枝的耗时\n",
    "duration = time.time() - start\n",
    "print(duration * 1000, 'ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论: 打印出来的bias_mask张量, 完全是按照预定义的方式每隔一位遮掩掉一位, 0和1交替出现, 后续执行remove操作的时候, 原始的bias_orig中的权重就会同样的被每隔一位剪枝掉一位."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

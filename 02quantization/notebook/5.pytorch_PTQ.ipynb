{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 环境配置\n",
    "\n",
    "首先，我们导入必须的环境，数据集和model使用和前几章相同的minist数据集和LeNet网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbfc409bc90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.quantization\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from utils import LeNet,train,evaluate,get_model_size,get_model_flops,MiB\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 构建模型和数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "    \n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "        x = self.maxpool2(self.relu2(self.conv2(x)))\n",
    "\n",
    "        x = x.contiguous().view(x.shape[0], -1)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.relu4(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (relu4): ReLU()\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3546/3723449019.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(data_dir+'/model.pt')\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet()#.to(device=device)\n",
    "print(model)\n",
    "# 加载模型的状态字典\n",
    "data_dir = \"../../01prune/notebook/0.minist_classify\"\n",
    "checkpoint = torch.load(data_dir+'/model.pt')\n",
    "# checkpoint = torch.load('./model.pt')\n",
    "\n",
    "# 加载状态字典到模型\n",
    "model.load_state_dict(checkpoint)\n",
    "fp32_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置归一化\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# 获取数据集\n",
    "calib_dataset = datasets.MNIST(root=data_dir+'/data/mnist', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=data_dir+'/data/mnist', train=False, download=True, transform=transform)  # train=True训练集，=False测试集\n",
    "# 设置DataLoader\n",
    "batch_size = 64\n",
    "calib_loader = DataLoader(calib_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 验证 FP32 模型的精度以及模型大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1022cadc70204c718dee536eecd09425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp32 model has accuracy=97.99000%\n",
      "fp32 model has size=0.17 MiB\n"
     ]
    }
   ],
   "source": [
    "fp32_model_accuracy = evaluate(fp32_model, test_loader)\n",
    "fp32_model_size = get_model_size(fp32_model)\n",
    "print(f\"fp32 model has accuracy={fp32_model_accuracy:.5f}%\")\n",
    "print(f\"fp32 model has size={fp32_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 将模型转化为量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qnnpack', 'none', 'onednn', 'x86', 'fbgemm']\n"
     ]
    }
   ],
   "source": [
    "backend = torch.backends.quantized.supported_engines #运行时可以使用这句命令检查自己支持的后端\n",
    "print(backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 使用测试集进行校准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/miniconda3/envs/pt11.8/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 配置量化\n",
    "quant_model = copy.deepcopy(fp32_model)\n",
    "quant_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "torch.quantization.prepare(quant_model, inplace=True)\n",
    "\n",
    "# 使用校准数据跑一遍前向传播\n",
    "def calibrate(model, loader, num_batches=batch_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            model(data)\n",
    "\n",
    "calibrate(quant_model, calib_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 验证量化模型的精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499cee12f40948a6aee1ff3787ab09bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant model has accuracy=97.99000%\n"
     ]
    }
   ],
   "source": [
    "quant_model_accuracy = evaluate(quant_model, test_loader)\n",
    "print(f\"quant model has accuracy={quant_model_accuracy:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 将量化模型转化为量化部署模型并验证其精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里重新定义一个验证函数。因为模型在量化后，输入输出也需要进行量化。具体添加的内容如下：\n",
    "    outputs = dequant(outputs)\n",
    "    inputs = quant(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "quant = torch.quantization.QuantStub() \n",
    "dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "def evaluate_quant(\n",
    "  model: nn.Module,\n",
    "  dataloader: DataLoader,\n",
    "  extra_preprocess = None\n",
    ") -> float:\n",
    "  model.eval()\n",
    "\n",
    "  num_samples = 0\n",
    "  num_correct = 0\n",
    "  \n",
    "  for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "    # Move the data from CPU to GPU\n",
    "    # inputs = inputs.to('mps')\n",
    "    if extra_preprocess is not None:\n",
    "        for preprocess in extra_preprocess:\n",
    "            inputs = quant(inputs)\n",
    "            inputs = preprocess(inputs)\n",
    "\n",
    "    # targets = targets.to('mps')\n",
    "\n",
    "    # Inference\n",
    "    outputs = model(inputs)\n",
    "    # print(outputs)\n",
    "    outputs = dequant(outputs)\n",
    "    # Convert logits to class indices\n",
    "    # print(outputs)\n",
    "    outputs = outputs.to(\"cpu\")\n",
    "    # outputs = outputs.argmax(dim=1)\n",
    "    outputs=torch.max(outputs,1)[1]\n",
    "\n",
    "    # Update metrics\n",
    "    num_samples += targets.size(0)\n",
    "    num_correct += (outputs == targets).sum()\n",
    "\n",
    "  return (num_correct / num_samples * 100).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将量化训练后的模型中的Fake节点去掉，这样可以得到一个真正的int8模型。下边模型中的节点算子都变成了Quantized算子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): QuantizedConv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.15729273855686188, zero_point=35)\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): QuantizedConv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.34748244285583496, zero_point=57)\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): QuantizedLinear(in_features=256, out_features=120, scale=0.2727542221546173, zero_point=51, qscheme=torch.per_channel_affine)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): QuantizedLinear(in_features=120, out_features=84, scale=0.22233228385448456, zero_point=52, qscheme=torch.per_channel_affine)\n",
      "  (relu4): ReLU()\n",
      "  (fc3): QuantizedLinear(in_features=84, out_features=10, scale=0.3006247580051422, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "  (quant): Quantize(scale=tensor([0.0255]), zero_point=tensor([17]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.quantization.convert(quant_model, inplace=True)\n",
    "print(quant_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证全int8模型的精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到全int8模型的精度和量化训练时的最后一个epoch的精度几乎一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c59bae9046a4a7092bc58f5bbe21dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_quant model has accuracy=97.88000%\n"
     ]
    }
   ],
   "source": [
    "final_quant_model_accuracy = evaluate_quant(quant_model, test_loader)\n",
    "print(f\"final_quant model has accuracy={final_quant_model_accuracy:.5f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_onnx(model, model_name):\n",
    "    dummy_input = torch.randn(1, 1, 28, 28)  \n",
    "    torch.onnx.export(model,\n",
    "                      dummy_input,\n",
    "                      f\"{model_name}.onnx\", \n",
    "                      input_names=[\"input\"],\n",
    "                      output_names=[\"output\"],\n",
    "                      dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存量化后的模型\n",
    "save_onnx(quant_model, \"quant_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存量化前的onnx模型\n",
    "save_onnx(fp32_model, \"fp32_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
